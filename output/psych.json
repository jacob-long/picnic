{
  "update": "2026-02-27",
  "content": [
    {
      "journal_full": "Advances in Methods and Practices in Psychological Science",
      "journal_short": "AMPPS",
      "articles": [
        {
          "title": "Beyond Statistical Myopia: Replying to a Misguided Critique of Mind–Body Research",
          "authors": "Peter J. Aungle, Daniel L. Chen, Nicholas P. Holmes",
          "abstract": "In response to Gelman and Brown’s recent critique of Aungle and Langer, we argue that their article illustrates how narrow statistical reasoning and selective literature review can misrepresent and undermine credible scientific findings. Using their discussion of perceived time and physical healing as a case study, we identify three general problems: (a) a failure to accurately characterize the methods and results of the study they critiqued, (b) misinterpretations and omissions in their review of the relevant literature, and (c) a tendency to generalize from isolated statistical issues to sweeping claims about the invalidity of mind–body research. We adopt Gelman and Brown’s recommended model and find that the main effect remains robust. We also document errors in their interpretations of other cited studies and demonstrate that they ignore decades of rigorous, well-replicated research on placebo effects and health mindsets. By examining their critique in detail, we highlight how methodological skepticism, when untethered from accurate reading and balanced appraisal, can mislead rather than clarify.",
          "url": "https://doi.org/10.1177/25152459261417257",
          "doi": "https://doi.org/10.1177/25152459261417257",
          "filter": 0,
          "created": "2026-2-25"
        },
        {
          "title": "Large Language Models as Psychological Simulators: A Methodological Guide",
          "authors": "Zhicheng Lin",
          "abstract": "Large language models (LLMs) offer emerging opportunities for psychological and behavioral research, but methodological guidance is lacking. In this article, I develop a framework for using LLMs as psychological simulators across two primary applications: simulating roles and personas to explore diverse contexts, and serving as computational models to investigate cognitive processes. For simulation, the framework includes (a) an implementation-confound checklist distinguishing essential from context-dependent methodological checks, (b) methods for developing psychologically grounded personas that move beyond demographic categories, and (c) a three-tier validation framework (direct, indirect, and generative) tailored to data availability. A diagnostic decision framework guides researchers through establishing performance validity, identifying implementation artifacts, and interpreting LLM-human discrepancies. For cognitive modeling, I synthesize (a) emerging approaches for probing internal representations, (b) methodological advances in causal interventions, and (c) strategies for relating model behavior to human cognition. The framework addresses overarching challenges, including prompt sensitivity, temporal limitations from training-data cutoffs, and ethical considerations that extend beyond traditional human-subjects review. Open-weight models are the default for reproducibility. Together, this framework integrates emerging empirical evidence about LLM performance—including systematic biases, cultural limitations, and prompt brittleness—to help researchers wrangle these challenges and leverage the unique capabilities of LLMs in psychological research.",
          "url": "https://doi.org/10.1177/25152459251410153",
          "doi": "https://doi.org/10.1177/25152459251410153",
          "filter": 0,
          "created": "2026-2-26"
        }
      ],
      "articles_hidden": []
    },
    {
      "journal_full": "Behavior Research Methods",
      "journal_short": "BRM",
      "articles": [
        {
          "title": "How plausible is my model? Assessing model plausibility of structural equation models using Bayesian posterior probabilities (BPP)",
          "authors": "Ivan Jacob Agaloos Pesigan, Shu Fai Cheung, Huiping Wu, Florbela Chang, Shing On Leung",
          "abstract": "In structural equation modeling (SEM), one method to select the most plausible model from several candidates, or to compare one or more hypothesized models with similar alternatives on plausibility, is to compare the models using Bayesian posterior probability (BPP). BPP can be computed from the Bayesian information criterion (BIC) scores (Wu et al. Multivariate Behavioral Research , 55 (1), 1–16, 2020). This approach complements conventional goodness-of-fit indices such as the Comparative Fit Index (CFI), the root mean square error of approximation (RMSEA), and the standardized root mean square residual (SRMR) in giving concise BPP for assessing uncertainties among all models considered. It can also reveal evidence against a model otherwise hidden by these indices. However, Wu et al. Multivariate Behavioral Research , 55 (1), 1–16. (2020) did not provide guidelines on deciding the models that should be considered. To facilitate the use of BPP, we proposed a novel method for selecting this set of models, called neighboring models , to help researchers decide on the initial set. This novel method integrates seamlessly into the typical workflow for SEM analysis. Researchers can fit a model as usual and then use this method to assess whether it is the most plausible model compared with the neighboring models. We believe the proposed method will make it easier for researchers to make better-informed decisions when evaluating their models. We developed a user-friendly R package, , to automate all the steps: generating the set of neighboring models, fitting them, and computing the BPPs, all in a single function.",
          "url": "https://doi.org/10.3758/s13428-025-02921-x",
          "doi": "https://doi.org/10.3758/s13428-025-02921-x",
          "filter": 0,
          "created": "2026-2-23"
        },
        {
          "title": "A tutorial for software options to aid in assessing functional relations in single-case experimental designs",
          "authors": "Rumen Manolov",
          "abstract": "Single-case experimental designs (SCEDs) can be used for identifying effective interventions via the intensive study of one or a few individuals in different conditions, actively manipulated by the researcher. The assessment of SCED data entails both judging whether there is sufficient evidence of a functional relation (i.e., a causal effect of the intervention on the target behavior) and quantifying the magnitude of the effect. In the current text, the focus is on assessing the presence of a functional relation, considering all the attempts to demonstrate an effect that SCEDs include. Specifically, the aim is to review several freely available websites, which require no additional software to be installed, and offer graphical representations of the data, visual aids, and quantifications. Several data analytical steps are outlined for performing the assessment, both dealing with each basic effect separately and evaluating the consistency of effects. Software that is useful for carrying out these steps is reviewed, including the way in which the data files should be specified and the few clicks required by applied researchers to achieve the desired output. The interpretations of the output are illustrated with real data.",
          "url": "https://doi.org/10.3758/s13428-026-02951-z",
          "doi": "https://doi.org/10.3758/s13428-026-02951-z",
          "filter": 0,
          "created": "2026-2-23"
        },
        {
          "title": "Generalized least squares transformation for single-case experimental design: Introducing the R package lmeSCED",
          "authors": "Chendong Li, Eunkyeng Baek, Wen Luo",
          "url": "https://doi.org/10.3758/s13428-025-02936-4",
          "doi": "https://doi.org/10.3758/s13428-025-02936-4",
          "filter": 0,
          "created": "2026-2-23"
        },
        {
          "title": "Comparing effect latencies in the visual world paradigm: Monte Carlo simulations to assess resampling-based procedures",
          "authors": "Serge Minor",
          "abstract": "In a series of Monte Carlo simulation studies, we evaluated the power and Type I error rates of resampling-based procedures for comparing effect latencies between groups in the visual world paradigm (VWP). Resampling-based methods, while versatile, are known to fail in certain cases. Therefore, validation of such methods through simulation is crucial. We compared permutation- and bootstrapping-based tests combined with different methods for measuring effect latency while manipulating sample size and true effect size. Alongside previously used latency measures, we tested new measures involving the application of an effect size threshold. Simulations were based on existing VWP datasets representing different effect types (preferential looks triggered by lexical vs. grammatical cues, cohort competitor effects in word recognition) and data collection methods (infrared- vs. webcam-based eye tracking). A total of 156,000 simulations were conducted across five studies, involving 548 million resampled datasets. The main findings are as follows: (1) With sufficient sample sizes, tests were effective in detecting latency differences of 200–300 ms in sentence processing tasks, and as small as 100 ms in word recognition. (2) The permutation test and bootstrapped percentile CIs exhibited the highest overall power without inflation of Type I error rates. (3) Applying an effect size threshold in latency estimation led to consistent increases in statistical power. (4) Resampling by participant was robust to increases in cross-subject variability;in contrast, bootstrapping within participants and time bins led to elevated Type I error rates. Based on these results, we offer recommendations for using non-parametric resampling-based procedures to compare group latencies in VWP experiments.",
          "url": "https://doi.org/10.3758/s13428-025-02934-6",
          "doi": "https://doi.org/10.3758/s13428-025-02934-6",
          "filter": 0,
          "created": "2026-2-23"
        },
        {
          "title": "Collection of body–object interaction ratings for 5,637 Japanese words",
          "authors": "Masaya Mochizuki, Naoto Ota",
          "url": "https://doi.org/10.3758/s13428-025-02939-1",
          "doi": "https://doi.org/10.3758/s13428-025-02939-1",
          "filter": 0,
          "created": "2026-2-24"
        },
        {
          "title": "ConversationAlign: Open-source software for analyzing patterns of lexical use and alignment in conversation transcripts",
          "authors": "Benjamin Sacks, Virginia Ulichney, Anna Duncan, Chelsea Helion, Sarah M. Weinstein, Tania Giovannetti, Gus Cooney, Jamie Reilly",
          "abstract": "Much of our scientific understanding of language processing has been informed by controlled experiments divorced from the real-world demands of naturalistic communication. Conversation requires synchronization of rate, amplitude, lexical complexity, affective coloring, shared reference, and countless other verbal and nonverbal dimensions. Conversation is not merely a vector for information transfer but also serves as a mechanism for establishing or maintaining social relationships. This process of language calibration between interlocutors is known as linguistic alignment . We developed an open-source R package, ConversationAlign , capable of computing novel indices of linguistic alignment and main effects of language use between interlocutors by evaluating word choice across numerous semantic, affective, and lexical dimensions (e.g., valence, concreteness, frequency, word length). We describe the operations of ConversationAlign, including its primary functions of cleaning and transforming raw language data into simultaneous time series objects aggregated by interlocutor, turn, and conversation. We then outline mathematical operations involved in computing complementary indices of linguistic alignment that capture both local (synchrony in turn-by-turn scores) and global relations (overall proximity) between interlocutors. We present a use case of ConversationAlign applied to interview transcripts from American radio legend Terry Gross and her many guests spanning 15 years. We identify caveats for use and potential sources of bias (e.g., polysemy, missing data, robustness to brief language samples) and close with a discussion of potential applications to other populations. ConversationAlign (v 0.4.0) is freely available for download and use via CRAN or GitHub. For technical instructions and download, visit https://github.com/Reilly-ConceptsCognitionLab/ConversationAlign .",
          "url": "https://doi.org/10.3758/s13428-026-02954-w",
          "doi": "https://doi.org/10.3758/s13428-026-02954-w",
          "filter": 0,
          "created": "2026-2-20"
        },
        {
          "title": "Quantifying the stability landscapes of psychological networks",
          "authors": "Jingmeng Cui, Gabriela Lunansky, Anna Lichtwarck-Aschoff, Norman B. Mendoza, Fred Hasselman",
          "abstract": "The network theory of psychopathology proposes that mental disorders can be represented as networks of interacting psychiatric symptoms. These direct symptom–symptom interactions can create a vicious cycle of symptom activation, pushing the network to a self-sustaining, dysfunctional phase of psychopathology: a mental disorder. Symptom network models can be estimated from empirical data through statistical models. Although simulation studies have established a relation between the structure of these symptom network models and the probability they end up in a self-sustaining dysfunctional phase, the general stability of the system is left implicit. The general stability includes both the stability of the dysfunctional phase and the stability of the healthy phase. In this paper, we present a novel method to quantify the stability landscapes of network models through stability landscapes. Our method is based on the Hamiltonian of the microstates of Ising models and can be used to show the stability of estimated Ising network models. Compared to simulation-based methods, our approach is computationally more efficient and quantifies the stability of all possible system states. Furthermore, we propose a set of stability metrics to quantify the stability of the healthy and dysfunctional phases and a bootstrapping method for range estimation of the stability metrics. To demonstrate the method’s utility, we apply it to an empirical data set and show how it can be used to compare the stability of phases between groups. The presented method is implemented in a freely available R package, Isinglandr .",
          "url": "https://doi.org/10.3758/s13428-025-02917-7",
          "doi": "https://doi.org/10.3758/s13428-025-02917-7",
          "filter": 0,
          "created": "2026-2-20"
        }
      ],
      "articles_hidden": []
    },
    {
      "journal_full": "Computers in Human Behavior",
      "journal_short": "CHB",
      "articles": [
        {
          "title": "Engaging with cybercriminals: phases and influence strategies in ransomware negotiations.",
          "authors": "Michail Georgiou, Ellen Giebels, Miriam S.D. Oostinga, Remco Spithoven",
          "url": "https://doi.org/10.1016/j.chb.2026.108953",
          "doi": "https://doi.org/10.1016/j.chb.2026.108953",
          "filter": 0,
          "created": "2026-2-26"
        },
        {
          "title": "Virtual peers reduce gambling symptoms and related problems of moderate-risk gamblers: A randomized controlled trial",
          "authors": "Kenji Yokotani, Yosuke Seki, Nobuhito Abe, Masahiro Takamura, Tetsuya Yamamoto, Hideyuki Takahashi",
          "url": "https://doi.org/10.1016/j.chb.2026.108956",
          "doi": "https://doi.org/10.1016/j.chb.2026.108956",
          "filter": 0,
          "created": "2026-2-22"
        },
        {
          "title": "AI chatbots in mental Health: How emojis, prompt type, and interactivity shape user perceptions in the United States and China",
          "authors": "Jihye Lee, Zinan Darren Yang, Weijia Shi, Yan Liu",
          "url": "https://doi.org/10.1016/j.chb.2026.108955",
          "doi": "https://doi.org/10.1016/j.chb.2026.108955",
          "filter": 0,
          "created": "2026-2-20"
        },
        {
          "title": "Interdisciplinary perspectives and current findings on the role of trust as a psychological mediator in human interaction with artificial intelligence: Editorial overview",
          "authors": "Irene Valori, Johannes Kraus, Merle T. Fairhurst",
          "url": "https://doi.org/10.1016/j.chb.2026.108957",
          "doi": "https://doi.org/10.1016/j.chb.2026.108957",
          "filter": 0,
          "created": "2026-2-21"
        }
      ],
      "articles_hidden": []
    },
    {
      "journal_full": "Journal of Experimental Social Psychology",
      "journal_short": "JESP",
      "articles": [
        {
          "title": "Navigating ideological divides in digital spaces: How political ideology and moral rhetoric shape the promotion of causes online",
          "authors": "Monica Gamez-Djokic, Marlon Mooijman, Matthew D. Rocklage, Maryam Kouchaki",
          "url": "https://doi.org/10.1016/j.jesp.2026.104893",
          "doi": "https://doi.org/10.1016/j.jesp.2026.104893",
          "filter": 0,
          "created": "2026-2-20"
        },
        {
          "title": "Principles of nostalgia: Meta-analytic tests",
          "authors": "Evan Weingarten, Ziwei Wei, Tim Wildschut, Constantine Sedikides",
          "url": "https://doi.org/10.1016/j.jesp.2026.104891",
          "doi": "https://doi.org/10.1016/j.jesp.2026.104891",
          "filter": 0,
          "created": "2026-2-26"
        }
      ],
      "articles_hidden": []
    },
    {
      "journal_full": "Journal of Personality and Social Psychology",
      "journal_short": "JPSP",
      "articles": [
        {
          "title": "Are the metatraits fact or artifact? Ruling out alternative explanations for the higher-order factors of the Big Five.",
          "authors": "Colin G. DeYoung, Ming Him Tai, Edward Chou, Boris Mlačić",
          "url": "https://doi.org/10.1037/pspp0000593",
          "doi": "https://doi.org/10.1037/pspp0000593",
          "filter": 0,
          "created": "2026-2-23"
        },
        {
          "title": "Confront in public, validate in private: Effective male allyship responses to sexist remarks.",
          "authors": "Hsuan-Che (Brad) Huang, Jonathan B. Evans",
          "url": "https://doi.org/10.1037/pspi0000515",
          "doi": "https://doi.org/10.1037/pspi0000515",
          "filter": 0,
          "created": "2026-2-26"
        },
        {
          "title": "Transcending embarrassment: On the reputational benefits of laughing at yourself.",
          "authors": "Selin Goksel, Ovul Sezer, Jonathan Z. Berman",
          "url": "https://doi.org/10.1037/pspa0000477",
          "doi": "https://doi.org/10.1037/pspa0000477",
          "filter": 0,
          "created": "2026-2-26"
        },
        {
          "title": "Femininity culture: Theory and workplace implications.",
          "authors": "Andrea C. Vial, Marta Beneda",
          "url": "https://doi.org/10.1037/pspi0000514",
          "doi": "https://doi.org/10.1037/pspi0000514",
          "filter": 0,
          "created": "2026-2-23"
        }
      ],
      "articles_hidden": []
    },
    {
      "journal_full": "Multivariate Behavioral Research",
      "journal_short": "MBR",
      "articles": [
        {
          "title": "Penalized Subgrouping of Heterogeneous Time Series",
          "authors": "Christopher M Crawford, Jonathan J Park, Sy-Miin Chow, Anja F Ernst, Vladas Pipiras, Zachary F Fisher",
          "url": "https://doi.org/10.1080/00273171.2026.2622120",
          "doi": "https://doi.org/10.1080/00273171.2026.2622120",
          "filter": 0,
          "created": "2026-2-20"
        }
      ],
      "articles_hidden": []
    },
    {
      "journal_full": "Personality and Social Psychology Bulletin",
      "journal_short": "PSPB",
      "articles": [
        {
          "title": "The Gendered Benefits of Communication Strategies: Women Leaders Are Less Effective but More Liked When They Use Prevention-Focused Language",
          "authors": "M. Asher Lawson, Sandra C. Matz, Friedrich M. Götz, Ashley E. Martin",
          "abstract": "Research has identified a double-bind for female leaders: When acting in line with gender stereotypes, they are viewed as more likeable but less competent. Here, we test the impact of using gender stereotypical language—characterized by more prevention-focused language (e.g., avoiding risks) and less promotion-focused language (e.g., seeking gains)—on U.S. governors’ approval ratings during COVID-19 and their ability to promote effective social distancing behaviors. With a final dataset of 3,759 documents capturing governors’ communication, a 13-week panel of Google mobility data containing 6,534 observations (Study 1), U.S. nationally representative survey data from 57,532 participants (Study 2), and 24,247 tweets (Study 3), we find that female governors who use less prevention-focused, stereotypical language in their communications are more effective at increasing compliance with social distancing measures but receive lower approval ratings. As such, women leaders’ necessary approaches in crisis situations may undermine their sustainability in positions of power.",
          "url": "https://doi.org/10.1177/01461672261420854",
          "doi": "https://doi.org/10.1177/01461672261420854",
          "filter": 0,
          "created": "2026-2-23"
        }
      ],
      "articles_hidden": []
    }
  ]
}
