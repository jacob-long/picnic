{
  "update": "2026-02-26",
  "content": [
    {
      "journal_full": "Advances in Methods and Practices in Psychological Science",
      "journal_short": "AMPPS",
      "articles": [
        {
          "title": "Beyond Statistical Myopia: Replying to a Misguided Critique of Mind–Body Research",
          "authors": "Peter J. Aungle, Daniel L. Chen, Nicholas P. Holmes",
          "abstract": "In response to Gelman and Brown’s recent critique of Aungle and Langer, we argue that their article illustrates how narrow statistical reasoning and selective literature review can misrepresent and undermine credible scientific findings. Using their discussion of perceived time and physical healing as a case study, we identify three general problems: (a) a failure to accurately characterize the methods and results of the study they critiqued, (b) misinterpretations and omissions in their review of the relevant literature, and (c) a tendency to generalize from isolated statistical issues to sweeping claims about the invalidity of mind–body research. We adopt Gelman and Brown’s recommended model and find that the main effect remains robust. We also document errors in their interpretations of other cited studies and demonstrate that they ignore decades of rigorous, well-replicated research on placebo effects and health mindsets. By examining their critique in detail, we highlight how methodological skepticism, when untethered from accurate reading and balanced appraisal, can mislead rather than clarify.",
          "url": "https://doi.org/10.1177/25152459261417257",
          "doi": "https://doi.org/10.1177/25152459261417257",
          "filter": 0,
          "created": "2026-2-25"
        },
        {
          "title": "Using Heteroskedasticity-Consistent Standard Errors and the Bootstrap for Linear Regression Analysis Available in SPSS: A Tutorial",
          "authors": "Hanna Rajh-Weber, Stefan Ernest Huber, Martin Arendasy",
          "abstract": "In the landscape of statistical software, from customizable programming-language-based to point-and-click systems, SPSS remains a popular choice among researchers. In SPSS, analyses with conventional methods, such as ordinary least squares regression, can be easily performed. However, violated assumptions, such as homoskedasticity or normality of the errors, can lead to altered Type I error rates or a reduction in statistical power. SPSS provides a multitude of alternative inference methods associated with linear regression, but accessing them is not always straightforward. To facilitate data analysis when assumptions for conventional inference methods are not met, in this tutorial, we aim to provide applied researchers, particularly SPSS users, with a guide for performing linear regression analyses using heteroskedasticity-consistent (HC) standard errors (HC3 and HC4) and two different bootstrap resampling methods (pairs bootstrap and wild bootstrap). Each bootstrap method can further be combined with a bootstrap p value, a percentile confidence interval, or a bias-corrected and accelerated confidence interval. For illustration, the methods are then compared using a computer-generated data set. Although the focus of this article is on applied researchers who use mainly SPSS for their analyses, a tutorial on how to do everything shown here in R (with custom functions) is included in the supplementary materials.",
          "url": "https://doi.org/10.1177/25152459251408046",
          "doi": "https://doi.org/10.1177/25152459251408046",
          "filter": 0,
          "created": "2026-2-19"
        },
        {
          "title": "Handling Item-Level Missing Data in Linear Regression: A Tutorial",
          "authors": "Guyin Zhang, Lihan Chen, Dexin Shi",
          "abstract": "With advances in methodology and statistical software, modern methods for handling missing data have become more accessible and straightforward to apply. In psychological studies, researchers often use questionnaires or scales composed of multiple items to measure constructs of interest. As a result, missing values frequently occur at the item level, whereas data analyses are typically conducted at the scale (composite) level. However, properly addressing item-level missing data remains a common challenge for many applied psychologists, including researchers who are otherwise well experienced in handling missing data at the scale level. In this tutorial, we introduce six approaches for handling item-level missing data: listwise deletion, hybrid methods that include proration with listwise deletion and proration with full-information maximum likelihood, item-level full-information maximum likelihood, item-level multiple imputation, two-stage maximum likelihood, and composite score factored regression. Using a published empirical data set, we provide step-by-step guidance on applying these methods in linear regression models. We include R code for each method and corresponding Mplus syntax if applicable. Finally, we summarize the key assumptions, advantages, and limitations of each approach and offer practical recommendations for researchers.",
          "url": "https://doi.org/10.1177/25152459261416497",
          "doi": "https://doi.org/10.1177/25152459261416497",
          "filter": 0,
          "created": "2026-2-19"
        },
        {
          "title": "When Do Interaction/Moderation Effects Stabilize in Linear Regression?",
          "authors": "Andrew Castillo, Joshua D. Miller, Colin Vize, David A. A. Baranger, Donald R. Lynam",
          "abstract": "Two-way interaction effects in linear regression occur when the relation between two variables changes depending on the level of a third. Despite their frequent use, interactions are notoriously difficult to estimate accurately and test for statistical significance because of small effect sizes and low reliability. In this study, we used Monte Carlo simulations to establish stability thresholds for two-way interactions between continuous variables across combinations of reliability (0.7–1.0), main effect size (0.1–0.5), collinearity (0.1–0.5), and interaction effect size (0.05–0.2). Stability was defined as the consistency of estimated effect sizes across repeated samples of the same size from the same population and operationalized using modified definitions of the corridor of stability and point of stability from Schönbrodt and Perugini. Results show that the stability of interaction estimates is primarily determined by sample size and predictor reliability. The case representing a realistic psychology field study, in which researchers have limited control over variables, stabilized at n = 3,800, requiring 72% statistical power. At n ≤≤ 100, 11% to 45% of the estimates were incorrectly signed (i.e., negative when the true effect was positive). Most psychology studies enroll far fewer than 500 participants, and our results indicate many published interactions may be unstable. Analyses involving highly reliable predictors, such as group assignment in experimental designs, may stabilize at lower sample sizes because they attenuate the expected effect size less than variables with more measurement error. Researchers are encouraged to avoid routine tests of two-way interactions unless sample size and reliability are adequate and hypotheses are specified a priori.",
          "url": "https://doi.org/10.1177/25152459251407860",
          "doi": "https://doi.org/10.1177/25152459251407860",
          "filter": 0,
          "created": "2026-2-19"
        }
      ],
      "articles_hidden": []
    },
    {
      "journal_full": "Behavior Research Methods",
      "journal_short": "BRM",
      "articles": [
        {
          "title": "How plausible is my model? Assessing model plausibility of structural equation models using Bayesian posterior probabilities (BPP)",
          "authors": "Ivan Jacob Agaloos Pesigan, Shu Fai Cheung, Huiping Wu, Florbela Chang, Shing On Leung",
          "abstract": "In structural equation modeling (SEM), one method to select the most plausible model from several candidates, or to compare one or more hypothesized models with similar alternatives on plausibility, is to compare the models using Bayesian posterior probability (BPP). BPP can be computed from the Bayesian information criterion (BIC) scores (Wu et al. Multivariate Behavioral Research , 55 (1), 1–16, 2020). This approach complements conventional goodness-of-fit indices such as the Comparative Fit Index (CFI), the root mean square error of approximation (RMSEA), and the standardized root mean square residual (SRMR) in giving concise BPP for assessing uncertainties among all models considered. It can also reveal evidence against a model otherwise hidden by these indices. However, Wu et al. Multivariate Behavioral Research , 55 (1), 1–16. (2020) did not provide guidelines on deciding the models that should be considered. To facilitate the use of BPP, we proposed a novel method for selecting this set of models, called neighboring models , to help researchers decide on the initial set. This novel method integrates seamlessly into the typical workflow for SEM analysis. Researchers can fit a model as usual and then use this method to assess whether it is the most plausible model compared with the neighboring models. We believe the proposed method will make it easier for researchers to make better-informed decisions when evaluating their models. We developed a user-friendly R package, , to automate all the steps: generating the set of neighboring models, fitting them, and computing the BPPs, all in a single function.",
          "url": "https://doi.org/10.3758/s13428-025-02921-x",
          "doi": "https://doi.org/10.3758/s13428-025-02921-x",
          "filter": 0,
          "created": "2026-2-23"
        },
        {
          "title": "A tutorial for software options to aid in assessing functional relations in single-case experimental designs",
          "authors": "Rumen Manolov",
          "abstract": "Single-case experimental designs (SCEDs) can be used for identifying effective interventions via the intensive study of one or a few individuals in different conditions, actively manipulated by the researcher. The assessment of SCED data entails both judging whether there is sufficient evidence of a functional relation (i.e., a causal effect of the intervention on the target behavior) and quantifying the magnitude of the effect. In the current text, the focus is on assessing the presence of a functional relation, considering all the attempts to demonstrate an effect that SCEDs include. Specifically, the aim is to review several freely available websites, which require no additional software to be installed, and offer graphical representations of the data, visual aids, and quantifications. Several data analytical steps are outlined for performing the assessment, both dealing with each basic effect separately and evaluating the consistency of effects. Software that is useful for carrying out these steps is reviewed, including the way in which the data files should be specified and the few clicks required by applied researchers to achieve the desired output. The interpretations of the output are illustrated with real data.",
          "url": "https://doi.org/10.3758/s13428-026-02951-z",
          "doi": "https://doi.org/10.3758/s13428-026-02951-z",
          "filter": 0,
          "created": "2026-2-23"
        },
        {
          "title": "Generalized least squares transformation for single-case experimental design: Introducing the R package lmeSCED",
          "authors": "Chendong Li, Eunkyeng Baek, Wen Luo",
          "url": "https://doi.org/10.3758/s13428-025-02936-4",
          "doi": "https://doi.org/10.3758/s13428-025-02936-4",
          "filter": 0,
          "created": "2026-2-23"
        },
        {
          "title": "Comparing effect latencies in the visual world paradigm: Monte Carlo simulations to assess resampling-based procedures",
          "authors": "Serge Minor",
          "abstract": "In a series of Monte Carlo simulation studies, we evaluated the power and Type I error rates of resampling-based procedures for comparing effect latencies between groups in the visual world paradigm (VWP). Resampling-based methods, while versatile, are known to fail in certain cases. Therefore, validation of such methods through simulation is crucial. We compared permutation- and bootstrapping-based tests combined with different methods for measuring effect latency while manipulating sample size and true effect size. Alongside previously used latency measures, we tested new measures involving the application of an effect size threshold. Simulations were based on existing VWP datasets representing different effect types (preferential looks triggered by lexical vs. grammatical cues, cohort competitor effects in word recognition) and data collection methods (infrared- vs. webcam-based eye tracking). A total of 156,000 simulations were conducted across five studies, involving 548 million resampled datasets. The main findings are as follows: (1) With sufficient sample sizes, tests were effective in detecting latency differences of 200–300 ms in sentence processing tasks, and as small as 100 ms in word recognition. (2) The permutation test and bootstrapped percentile CIs exhibited the highest overall power without inflation of Type I error rates. (3) Applying an effect size threshold in latency estimation led to consistent increases in statistical power. (4) Resampling by participant was robust to increases in cross-subject variability;in contrast, bootstrapping within participants and time bins led to elevated Type I error rates. Based on these results, we offer recommendations for using non-parametric resampling-based procedures to compare group latencies in VWP experiments.",
          "url": "https://doi.org/10.3758/s13428-025-02934-6",
          "doi": "https://doi.org/10.3758/s13428-025-02934-6",
          "filter": 0,
          "created": "2026-2-23"
        },
        {
          "title": "Collection of body–object interaction ratings for 5,637 Japanese words",
          "authors": "Masaya Mochizuki, Naoto Ota",
          "url": "https://doi.org/10.3758/s13428-025-02939-1",
          "doi": "https://doi.org/10.3758/s13428-025-02939-1",
          "filter": 0,
          "created": "2026-2-24"
        },
        {
          "title": "ConversationAlign: Open-source software for analyzing patterns of lexical use and alignment in conversation transcripts",
          "authors": "Benjamin Sacks, Virginia Ulichney, Anna Duncan, Chelsea Helion, Sarah M. Weinstein, Tania Giovannetti, Gus Cooney, Jamie Reilly",
          "abstract": "Much of our scientific understanding of language processing has been informed by controlled experiments divorced from the real-world demands of naturalistic communication. Conversation requires synchronization of rate, amplitude, lexical complexity, affective coloring, shared reference, and countless other verbal and nonverbal dimensions. Conversation is not merely a vector for information transfer but also serves as a mechanism for establishing or maintaining social relationships. This process of language calibration between interlocutors is known as linguistic alignment . We developed an open-source R package, ConversationAlign , capable of computing novel indices of linguistic alignment and main effects of language use between interlocutors by evaluating word choice across numerous semantic, affective, and lexical dimensions (e.g., valence, concreteness, frequency, word length). We describe the operations of ConversationAlign, including its primary functions of cleaning and transforming raw language data into simultaneous time series objects aggregated by interlocutor, turn, and conversation. We then outline mathematical operations involved in computing complementary indices of linguistic alignment that capture both local (synchrony in turn-by-turn scores) and global relations (overall proximity) between interlocutors. We present a use case of ConversationAlign applied to interview transcripts from American radio legend Terry Gross and her many guests spanning 15 years. We identify caveats for use and potential sources of bias (e.g., polysemy, missing data, robustness to brief language samples) and close with a discussion of potential applications to other populations. ConversationAlign (v 0.4.0) is freely available for download and use via CRAN or GitHub. For technical instructions and download, visit https://github.com/Reilly-ConceptsCognitionLab/ConversationAlign .",
          "url": "https://doi.org/10.3758/s13428-026-02954-w",
          "doi": "https://doi.org/10.3758/s13428-026-02954-w",
          "filter": 0,
          "created": "2026-2-20"
        },
        {
          "title": "Quantifying the stability landscapes of psychological networks",
          "authors": "Jingmeng Cui, Gabriela Lunansky, Anna Lichtwarck-Aschoff, Norman B. Mendoza, Fred Hasselman",
          "abstract": "The network theory of psychopathology proposes that mental disorders can be represented as networks of interacting psychiatric symptoms. These direct symptom–symptom interactions can create a vicious cycle of symptom activation, pushing the network to a self-sustaining, dysfunctional phase of psychopathology: a mental disorder. Symptom network models can be estimated from empirical data through statistical models. Although simulation studies have established a relation between the structure of these symptom network models and the probability they end up in a self-sustaining dysfunctional phase, the general stability of the system is left implicit. The general stability includes both the stability of the dysfunctional phase and the stability of the healthy phase. In this paper, we present a novel method to quantify the stability landscapes of network models through stability landscapes. Our method is based on the Hamiltonian of the microstates of Ising models and can be used to show the stability of estimated Ising network models. Compared to simulation-based methods, our approach is computationally more efficient and quantifies the stability of all possible system states. Furthermore, we propose a set of stability metrics to quantify the stability of the healthy and dysfunctional phases and a bootstrapping method for range estimation of the stability metrics. To demonstrate the method’s utility, we apply it to an empirical data set and show how it can be used to compare the stability of phases between groups. The presented method is implemented in a freely available R package, Isinglandr .",
          "url": "https://doi.org/10.3758/s13428-025-02917-7",
          "doi": "https://doi.org/10.3758/s13428-025-02917-7",
          "filter": 0,
          "created": "2026-2-20"
        }
      ],
      "articles_hidden": []
    },
    {
      "journal_full": "Computers in Human Behavior",
      "journal_short": "CHB",
      "articles": [
        {
          "title": "Offline friendship conflict and adolescent Internet addiction: Indirect associations via self-esteem and the moderating role of clique-level norms",
          "authors": "Yanli Hou, Ruonan Guo, Shengcheng Song, Caina Li",
          "url": "https://doi.org/10.1016/j.chb.2026.108954",
          "doi": "https://doi.org/10.1016/j.chb.2026.108954",
          "filter": 0,
          "created": "2026-2-19"
        },
        {
          "title": "Virtual peers reduce gambling symptoms and related problems of moderate-risk gamblers: A randomized controlled trial",
          "authors": "Kenji Yokotani, Yosuke Seki, Nobuhito Abe, Masahiro Takamura, Tetsuya Yamamoto, Hideyuki Takahashi",
          "url": "https://doi.org/10.1016/j.chb.2026.108956",
          "doi": "https://doi.org/10.1016/j.chb.2026.108956",
          "filter": 0,
          "created": "2026-2-22"
        },
        {
          "title": "AI chatbots in mental Health: How emojis, prompt type, and interactivity shape user perceptions in the United States and China",
          "authors": "Jihye Lee, Zinan Darren Yang, Weijia Shi, Yan Liu",
          "url": "https://doi.org/10.1016/j.chb.2026.108955",
          "doi": "https://doi.org/10.1016/j.chb.2026.108955",
          "filter": 0,
          "created": "2026-2-20"
        },
        {
          "title": "Evaluating user performance with RAG-based generative AI: A scenario-based experiment on AI-assisted information retrieval",
          "authors": "Aktilek Sagynbayeva, Ajin Pyo, Sang-Hyeak Yoon, Sung-Byung Yang",
          "url": "https://doi.org/10.1016/j.chb.2026.108952",
          "doi": "https://doi.org/10.1016/j.chb.2026.108952",
          "filter": 0,
          "created": "2026-2-19"
        },
        {
          "title": "Interdisciplinary Perspectives and Current Findings on the Role of Trust as a Psychological Mediator in Human Interaction with Artificial Intelligence: Editorial Overview",
          "authors": "Irene Valori, Johannes Kraus, Merle T. Fairhurst",
          "url": "https://doi.org/10.1016/j.chb.2026.108957",
          "doi": "https://doi.org/10.1016/j.chb.2026.108957",
          "filter": 0,
          "created": "2026-2-21"
        }
      ],
      "articles_hidden": []
    },
    {
      "journal_full": "Group Processes & Intergroup Relations",
      "journal_short": "GPIR",
      "articles": [
        {
          "title": "New kinds of group complexity in intergroup relations: An analysis of gender and sexuality",
          "authors": "Valentina Palacio Posada, Daniel J. Chiacchia, Geoffrey J. Leonardelli",
          "abstract": "Recognizing social identity complexity as one form of group complexity, we introduce two new kinds. Intergroup complexity encompasses perceived overlap between the ingroup and outgroup(s), whereas outgroup complexity entails overlap among outgroups (greater overlap yields simpler perceptions). Both apply to domains with at least one ingroup and two outgroups. Testing ideas from the social identity, gender, and sexuality relations literatures, we collected peoples’ perceptions of intergroup and outgroup complexity among gender and sexuality categories separately, using an online survey with a convenience sample of American adults ( N = 287). Results revealed that people perceived greater intergroup than outgroup complexity, less sexual than gender complexity (especially so among sexual outgroups), and were more likely to report greater intergroup complexity as their ingroup’s status increased. Moreover, social dominance orientation moderated status effects. Implications focus on the applicability of these new forms of group complexity and their consequences.",
          "url": "https://doi.org/10.1177/13684302261419401",
          "doi": "https://doi.org/10.1177/13684302261419401",
          "filter": 0,
          "created": "2026-2-19"
        },
        {
          "title": "The moderating role of collective narcissism in White Americans’ psychological defensiveness to the history of racism",
          "authors": "H. Annie Vu, Luis M. Rivera",
          "abstract": "Since teaching about past racism in the United States often necessitates deliberations over White Americans’ ingroup transgressions, it can elicit historical defensiveness. We tested this hypothesis across three experiments ( N s = 109, 263, and 601) and further investigated if this effect was moderated by White collective narcissism. White American participants were randomly assigned to an ingroup transgression (presented with the history of racism) or an ingroup nontransgression (presented with the history of general events) condition. Across all experiments, (a) facing ingroup transgressions increased perceived ingroup responsibility among participants with low collective narcissism but not among those with high collective narcissism, and (b) among participants facing ingroup transgressions only, strong collective narcissism was consistently associated with less perceived ingroup responsibility. This research highlights the potential dangers of collective narcissism in erasing the history of racism.",
          "url": "https://doi.org/10.1177/13684302261418031",
          "doi": "https://doi.org/10.1177/13684302261418031",
          "filter": 0,
          "created": "2026-2-19"
        },
        {
          "title": "Perceptions of an ally who confronts racism: The role of displayed emotion and response type",
          "authors": "Adriana Lopez, Cheryl L. Dickter",
          "abstract": "Previous research has demonstrated that when White allies confront racist comments, it reduces future prejudicial behavior in perpetrators, establishes egalitarian norms, and has positive effects for confronters. The current studies sought to examine whether the delivery of the confrontation affects perceptions of these allies. Three studies examined whether emotional expression (angry vs. control, Study 1), response type (direct vs. indirect, Study 2), and the interaction of these factors (Study 3) affected perceptions of White allies. Participants ( N = 740) evaluated a White person in a vignette who confronted a racist comment. Results indicated that confronters who expressed anger were viewed more negatively than those who did not, due to perceptions that they were motivated to make the perpetrator look bad. Direct responses also elicited more positive perceptions of the confronter than indirect responses. These results may inform educational strategies that encourage allies to confront racist remarks.",
          "url": "https://doi.org/10.1177/13684302261422160",
          "doi": "https://doi.org/10.1177/13684302261422160",
          "filter": 0,
          "created": "2026-2-19"
        },
        {
          "title": "Ignorance of history, political ideology, and attitudes toward Confederate symbols in the United States",
          "authors": "Tyler J. Robinson, Sydney M. Rivera, Ethan Zell",
          "abstract": "Conservatives report much more favorable attitudes toward Confederate symbols (e.g., flags, monuments) than liberals. However, little work has examined factors that mediate or explain this robust political difference. Across two studies, we explored whether knowledge of historical racism mediates political differences in Confederate symbol attitudes. In a predominantly White internet sample ( N = 227, U.S. South), Study 1 found that the association between political conservatism and attitudes toward Confederate symbols was mediated by historical knowledge. Further, this mediation effect remained after adjusting for Southern identity. Study 2 replicated the predicted mediation effect in a racially diverse university sample and found that it obtains across racial-ethnic groups ( N = 557, U.S. Southeast). These results suggest that ignorance of historical racism helps to explain political differences in Confederate symbol attitudes. We discuss implications of these findings for research on the connection between historical knowledge and racial attitudes (i.e., the Marley hypothesis).",
          "url": "https://doi.org/10.1177/13684302261419329",
          "doi": "https://doi.org/10.1177/13684302261419329",
          "filter": 0,
          "created": "2026-2-19"
        }
      ],
      "articles_hidden": []
    },
    {
      "journal_full": "Journal of Experimental Social Psychology",
      "journal_short": "JESP",
      "articles": [
        {
          "title": "Navigating ideological divides in digital spaces: How political ideology and moral rhetoric shape the promotion of causes online",
          "authors": "Monica Gamez-Djokic, Marlon Mooijman, Matthew D. Rocklage, Maryam Kouchaki",
          "url": "https://doi.org/10.1016/j.jesp.2026.104893",
          "doi": "https://doi.org/10.1016/j.jesp.2026.104893",
          "filter": 0,
          "created": "2026-2-20"
        }
      ],
      "articles_hidden": []
    },
    {
      "journal_full": "Journal of Personality and Social Psychology",
      "journal_short": "JPSP",
      "articles": [
        {
          "title": "Are the metatraits fact or artifact? Ruling out alternative explanations for the higher-order factors of the Big Five.",
          "authors": "Colin G. DeYoung, Ming Him Tai, Edward Chou, Boris Mlačić",
          "url": "https://doi.org/10.1037/pspp0000593",
          "doi": "https://doi.org/10.1037/pspp0000593",
          "filter": 0,
          "created": "2026-2-23"
        },
        {
          "title": "Femininity culture: Theory and workplace implications.",
          "authors": "Andrea C. Vial, Marta Beneda",
          "url": "https://doi.org/10.1037/pspi0000514",
          "doi": "https://doi.org/10.1037/pspi0000514",
          "filter": 0,
          "created": "2026-2-23"
        }
      ],
      "articles_hidden": []
    },
    {
      "journal_full": "Multivariate Behavioral Research",
      "journal_short": "MBR",
      "articles": [
        {
          "title": "Penalized Subgrouping of Heterogeneous Time Series",
          "authors": "Christopher M Crawford, Jonathan J Park, Sy-Miin Chow, Anja F Ernst, Vladas Pipiras, Zachary F Fisher",
          "url": "https://doi.org/10.1080/00273171.2026.2622120",
          "doi": "https://doi.org/10.1080/00273171.2026.2622120",
          "filter": 0,
          "created": "2026-2-20"
        }
      ],
      "articles_hidden": []
    },
    {
      "journal_full": "Personality and Social Psychology Bulletin",
      "journal_short": "PSPB",
      "articles": [
        {
          "title": "The Gendered Benefits of Communication Strategies: Women Leaders Are Less Effective but More Liked When They Use Prevention-Focused Language",
          "authors": "M. Asher Lawson, Sandra C. Matz, Friedrich M. Götz, Ashley E. Martin",
          "abstract": "Research has identified a double-bind for female leaders: When acting in line with gender stereotypes, they are viewed as more likeable but less competent. Here, we test the impact of using gender stereotypical language—characterized by more prevention-focused language (e.g., avoiding risks) and less promotion-focused language (e.g., seeking gains)—on U.S. governors’ approval ratings during COVID-19 and their ability to promote effective social distancing behaviors. With a final dataset of 3,759 documents capturing governors’ communication, a 13-week panel of Google mobility data containing 6,534 observations (Study 1), U.S. nationally representative survey data from 57,532 participants (Study 2), and 24,247 tweets (Study 3), we find that female governors who use less prevention-focused, stereotypical language in their communications are more effective at increasing compliance with social distancing measures but receive lower approval ratings. As such, women leaders’ necessary approaches in crisis situations may undermine their sustainability in positions of power.",
          "url": "https://doi.org/10.1177/01461672261420854",
          "doi": "https://doi.org/10.1177/01461672261420854",
          "filter": 0,
          "created": "2026-2-23"
        },
        {
          "title": "How Does Rejection Feel? Explaining Victims’ Reactions to Social Rejection From the Perspective of Self-Conscious Emotions",
          "authors": "Irene Castro, Saulo Fernández",
          "abstract": "Research has characterized the emotional response to social rejection as a generalized negative affect, overlooking the diverse reactions of rejected individuals. We explored how humiliation and related emotions (anger, shame, and guilt) are linked to post-rejection behavior, and how two key appraisals (unfairness and internalization of devaluation) evoke specific emotions. In two studies—an experimental Cyberball study ( N = 186) and a large-scale correlational study ( N = 1,200)—we found that humiliation was associated with both unfairness and internalization, anger only with unfairness, shame with internalization, and guilt with internalization and negatively with unfairness. Humiliation was correlated with aggressive confrontation and avoidance, anger with aggressive and non-aggressive confrontation, shame with avoidance and negatively with non-aggressive confrontation, and guilt with reparation and non-aggressive confrontation. We discuss the relevance of these emotional pathways for understanding social rejection and informing targeted interventions to mitigate harmful responses.",
          "url": "https://doi.org/10.1177/01461672261418066",
          "doi": "https://doi.org/10.1177/01461672261418066",
          "filter": 0,
          "created": "2026-2-19"
        },
        {
          "title": "Lay Attributions of Conspiracy Beliefs Predict Intentions to Correct Conspiracy Believers",
          "authors": "Valentin Mang, Kai Epstude, Bob M. Fennis",
          "abstract": "What do laypeople think causes conspiracy beliefs? In six correlational studies ( N = 2,024) and a qualitative study ( N = 190), we examined laypeople’s attributions of others’ conspiracy beliefs and how these attributions predict their intentions to correct conspiracy believers. Attribution research suggests that dispositional (vs. situational) attributions should dominate lay beliefs and negatively predict correction intentions. Dispositional attributions of conspiracy beliefs were indeed more prevalent than attributions to situational causes, with two exceptions: Conspiracy beliefs were attributed most strongly to influence from social media and misinformation. Attributing another person’s conspiracy beliefs more strongly to social media or misinformation also predicted intentions to correct this person, more so than other attributions. Our results suggest that (a) assessing attributions at a more detailed level than is often done can help uncover yet unobserved nuance in laypeople’s attributions and (b) encouraging certain attributions of conspiracy beliefs could help foster their interpersonal correction.",
          "url": "https://doi.org/10.1177/01461672261418068",
          "doi": "https://doi.org/10.1177/01461672261418068",
          "filter": 0,
          "created": "2026-2-19"
        }
      ],
      "articles_hidden": []
    },
    {
      "journal_full": "Psychological Science",
      "journal_short": "PsychSci",
      "articles": [
        {
          "title": "To Believe or Not to Believe in Conspiracy Claims? That Is a Question for Signal Detection Theory",
          "authors": "Maude Tagand, Dominique Muller, Cécile Nurra, Olivier Klein, Benjamin Aubert-Teillaud, Kenzo Nera",
          "abstract": "Conspiracy mentality is conceptualized as a continuum. Research on this topic has focused on unwarranted conspiracy claims and the upper end of the conspiracy-mentality continuum—people seeing conspiracies everywhere. This focus neglects warranted conspiracy claims and the lower end of the continuum. To better understand conspiracy mentality, we aimed to clarify both ends of the continuum using signal detection theory. We examined how people evaluate warranted and unwarranted conspiracy claims across levels of conspiracy mentality in two studies with 331 French-speaking adult participants from France, Switzerland, and Belgium (Study 1) and 576 English-speaking adult participants from the United States and the United Kingdom (Study 2), both groups recruited via Prolific. Compared with participants high in conspiracy mentality, those low in conspiracy mentality not only believed less in conspiracies but also underestimated their prevalence. However, participants low in conspiracy mentality were more accurate at distinguishing warranted from unwarranted conspiracy claims. These results provide a better understanding of conspiracy mentality and its relationship with the perceived truthfulness of conspiracies.",
          "url": "https://doi.org/10.1177/09567976261417245",
          "doi": "https://doi.org/10.1177/09567976261417245",
          "filter": 0,
          "created": "2026-2-19"
        }
      ],
      "articles_hidden": []
    },
    {
      "journal_full": "Psychology of Popular Media",
      "journal_short": "PPM",
      "articles": [
        {
          "title": "Effects of avatar behavior on aggression: Mediation of moral self-perception and moderation of avatar identification.",
          "authors": "Shupeng Heng, Ziwan Zhang, Danfeng Zheng",
          "url": "https://doi.org/10.1037/ppm0000652",
          "doi": "https://doi.org/10.1037/ppm0000652",
          "filter": 0,
          "created": "2026-2-19"
        }
      ],
      "articles_hidden": []
    }
  ]
}
