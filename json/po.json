{
  "update": "2026-02-03",
  "content": [
    {
      "journal_full": "Journal of Elections, Public Opinion and Parties",
      "journal_short": "JEPOP",
      "articles": [
        {
          "title": "Anger, negative partisanship, and joy in the suffering of political others",
          "authors": "Steven W. Webster, Mary Adams Plooster",
          "url": "https://doi.org/10.1080/17457289.2026.2624129",
          "doi": "https://doi.org/10.1080/17457289.2026.2624129",
          "filter": 0,
          "created": "2026-2-2"
        }
      ],
      "articles_hidden": []
    },
    {
      "journal_full": "Journal of Official Statistics",
      "journal_short": "JOS",
      "articles": [
        {
          "title": "How Does Noise Protection Affect the Accuracy of Life Expectancy and Other Demographic Indicators?",
          "authors": "Fabian Bach",
          "abstract": "New and efficient methods based on noise addition to protect the confidentiality in population statistics have been developed, tested, and applied in census production by various members of the European Statistical System over the past years. Basic demographic statistics—such as population stocks, live births and deaths by age, sex, and region—may be protected in a similar way, but also form the raw input to calculate various demographic indicators. This paper analyzes the impact on the accuracy of some selected indicators, namely fertility and mortality rates and life expectancies, under the assumption that the raw input counts are protected with a generic noise method with fixed variance parameter, by comparing the size of noise uncertainties with intrinsic statistical uncertainties using a Poisson model. As a by-product, we derive and validate numerically a closed analytical expression for the variance of life expectancies in a certain class of calculation models as a function of the variance of input mortality data. This expression also allows to calculate analytically the statistical uncertainty of life expectancies using the mentioned Poisson model for the input death counts.",
          "url": "https://doi.org/10.1177/0282423x251414001",
          "doi": "https://doi.org/10.1177/0282423x251414001",
          "filter": 0,
          "created": "2026-2-2"
        }
      ],
      "articles_hidden": []
    },
    {
      "journal_full": "Journal of Survey Statistics and Methodology",
      "journal_short": "JSSAM",
      "articles": [
        {
          "title": "Reducing the Bias From Probability and Nonprobability Online Panels by Excluding Satisficers",
          "authors": "Sam Slamowicz, Darren Pennay, Dina Neiger, Benjamin Phillips, Andrew C Ward, Michael Xu",
          "abstract": "Over the last 15 years or so, many studies have compared the accuracy of estimates produced when the same questionnaire is administered to samples generated using probability and nonprobability sampling methods. Most studies conclude that estimates from probability sample surveys are more accurate than estimates from nonprobability sample surveys. In trying to understand why probability sample surveys generally produce more accurate estimates than nonprobability sample surveys, most research has focused on the differences in survey samples reached as a result of the sampling mechanisms used by each approach, and looked to various adjustment (weighting) approaches to bring the nonprobability sample estimates closer to the comparative probability sample estimates and independent benchmarks. Less common, however, are comparative studies that focus on measurement error. This study compares estimates generated from five Australian online panels; one probability-based online panel and four nonprobability online panels. We use widely implemented techniques to identify instances of survey satisficing—the phenomenon of survey respondents providing lower quality responses to reduce cognitive effort. When comparing the level of satisficing across panels, a higher proportion of respondents in the nonprobability panels give responses consistent with satisficing than in the probability panel. We then show that excluding these suboptimal responses from our analysis dataset, before weighting, results in a greater bias reduction across the nonprobability panels than weighting alone, and that this is the case across all four nonprobability panels. In contrast, very little change to bias is observed after applying the same exclusions to the probability panel. We end with a call for others to undertake similar research in the hope that this approach might become part of the toolkit of techniques being developed to reduce the bias in survey estimates generated from nonprobability online samples.",
          "url": "https://doi.org/10.1093/jssam/smaf019",
          "doi": "https://doi.org/10.1093/jssam/smaf019",
          "filter": 0,
          "created": "2025-8-12"
        },
        {
          "title": "Estimating the Number of Street Vendors in New York City: Ratio Estimation with Point Process Data",
          "authors": "Jonathan Auerbach",
          "abstract": "We estimate the number of street vendors in New York City. First, we summarize the process by which vendors receive licenses and permits to operate legally in New York City. We then describe a survey that was administered by the Street Vendor Project while distributing coronavirus relief aid to vendors operating in New York City, both with and without a license or permit. Finally, we review ratio estimation and develop a theoretical justification based on the theory of point processes. We find approximately 23,000 street vendors operate in New York City—20,500 mobile food vendors and 2,400 general merchandise vendors—with one-third located in just six ZIP Codes—11368 (16 percent), 11372 (3 percent), and 11354 (3 percent) in North and West Queens and 10036 (5 percent), 10019 (4 percent), and 10001 (3 percent) in the Chelsea and Clinton neighborhoods of Manhattan. Our estimates suggest the American Community Survey misses the majority of New York City street vendors.",
          "url": "https://doi.org/10.1093/jssam/smaf051",
          "doi": "https://doi.org/10.1093/jssam/smaf051",
          "filter": 0,
          "created": "2025-11-21"
        },
        {
          "title": "Controlling Acquiescent Response Style Through Negated Versus Polar Opposite Items: Design Considerations for Balancing Measurement Scales",
          "authors": "Fernanda Alvarado-Leiton, Sunghee Lee, Rachel E Davis",
          "abstract": "Acquiescent response style (ARS) is the tendency to agree with measurement items regardless of their content. ARS tends to differ across cultures, making it particularly relevant for cross-cultural research, as it may disrupt cross-cultural comparability. Balanced measurement scales include items written in different directions of the measured construct and are hypothesized to be a remedy for undesirable effects of ARS. This study examined the effect on measurement properties of three scale balancing approaches in a cross-cultural research setting: (i) unbalanced scales; (ii) scale balancing with negated items where some of the items were written with a negation, such as “no” or “not”; and (iii) scale balancing that introduce items written with polar opposite terms (e.g. “unhappy” as the opposite of “satisfied”) that reversed the item direction without using negations. This study examined these approaches on four well-being measurement scales, originally unbalanced, that used an Agree-Disagree Likert response scale. Participants were recruited to a Web survey from three groups: U.S. non-Hispanic whites (n = 1,200, interviewed in English), U.S. Hispanics (n = 1,200, interviewed in English), and Mexicans (n = 1,200, interviewed in Spanish). Respondents were randomly assigned to one scale-balancing conditions for well-being measures. Balanced scales outperformed unbalanced scales for convergent validity, with higher correlations observed between scale scores and validation measures. Between negated and polar opposite balancing, no statistical differences were observed in factorial model fit, reliability, or convergent validity. This study supports that, when designed carefully, balanced measurement scales may be effective for addressing ARS and improving measurement properties over unbalanced scales.",
          "url": "https://doi.org/10.1093/jssam/smaf022",
          "doi": "https://doi.org/10.1093/jssam/smaf022",
          "filter": 0,
          "created": "2025-8-14"
        }
      ],
      "articles_hidden": []
    },
    {
      "journal_full": "Social Science Computer Review",
      "journal_short": "SSCR",
      "articles": [
        {
          "title": "α|D〉+β|H〉: Exploration of Quantum Deep Learning on Humanities Data from the Perspective of Digital Humanities",
          "authors": "Tao Fan, Hao Wang, Tobias Hodel",
          "abstract": "The emergence and advancement of new technologies present promising opportunities for data-driven research in the Digital Humanities (DH). As an innovative intersection of quantum computing and humanities data, Quantum Humanities (QH) holds significant research potential. However, current studies related to QH remain scarce with limited applications involving quantum deep learning, and its feasibility, effectiveness, and efficiency on humanities data need in-depth exploration. To address this gap, this paper takes the role type recognition of painted faces of Beijing Opera as a case study. Several classical and competitive quantum deep learning–based and classical deep learning–based models (e.g., quantum convolutional neural networks and classical convolutional neural networks) are selected for comparison on the public image dataset Painted Faces of Beijing Opera digitized from books. Extensive empirical experimental results demonstrate that quantum deep learning–based models are of certain advantages and hold promising prospects in applied DH practices.",
          "url": "https://doi.org/10.1177/08944393261416783",
          "doi": "https://doi.org/10.1177/08944393261416783",
          "filter": 0,
          "created": "2026-1-31"
        },
        {
          "title": "Harnessing Big Data, Hindered by Bias: Evaluating TikTok Research API for Fair and Optimal Social Sciences",
          "authors": "Dan Bai, Yan Gu",
          "abstract": "Digital platforms now serve as crucial archives for analysing societal trends, yet their research APIs pose methodological challenges. This study critically evaluates TikTok Research API through comparative analysis of 6,373 videos from 14 creators in the United States and United Kingdom (2020–2022), contrasting API-derived outputs with manual collection and third-party analytics. The API demonstrated scalability, retrieving more videos than alternative methods and providing 22 variables, including eight unavailable elsewhere. However, limitations were substantial: transcriptions covered about 10% of the content, with more transcripts returned from American male creators. Engagement metrics exhibited inconsistent accuracy across data sources, with the API showing systematically lower view counts but higher comment and share counts compared to manual collection. The number of videos varied depending on sample composition, indicating that small changes in inclusion criteria could shift outcomes disproportionately. These results highlight systematic inconsistencies, showing why multi-method approaches remain necessary despite automation. While TikTok Research API offers valuable scale and ethical compliance, its demographic biases and metadata inconsistencies compromise validity. The study advocates integrated auditing protocols and targeted API refinements to improve representativeness and accuracy in platform-based research.",
          "url": "https://doi.org/10.1177/08944393251413277",
          "doi": "https://doi.org/10.1177/08944393251413277",
          "filter": 0,
          "created": "2026-1-27"
        },
        {
          "title": "Conspiracy or Public Service? Does Facebook’s Third-Party Fact-Checking Increase Conspiracy Beliefs Among Americans?",
          "authors": "Justin Bonest Phillips, Timothy B. Gravelle, Andrea Carson, Mathew D. Marques",
          "abstract": "Unlike past studies that examine whether fact-checking can counter conspiratorial belief, we reverse the lens to investigate if fact-checking itself prompts conspiracy belief. Our study occurs in the days immediately preceding the 2024 US election. Shortly thereafter, Meta’s CEO Mark Zuckerberg abandoned Facebook’s third-party program altogether, arguing fact-checkers “have destroyed more trust than they have created.” We provide timely insight into fact-checking concerns using a preregistered online survey-based experiment of US Facebook users’ ( n = 2,409), randomly assigned to view either a generic Facebook fact-check (treatment) or a Facebook login screen. Results show no overall effects of third-party fact-checking on users’ propensity for conspiratorial beliefs. However, when individuals with high conspiracy mentality and strong conservative identification encounter a fact-check, they are more likely to endorse Facebook-related conspiracy beliefs. We also observe a three-way interaction among political independents with high and low conspiracy beliefs, where fact-checking potentially triggers or reduces such beliefs.",
          "url": "https://doi.org/10.1177/08944393261421119",
          "doi": "https://doi.org/10.1177/08944393261421119",
          "filter": 0,
          "created": "2026-1-29"
        },
        {
          "title": "Platform Politics in the U.S. Congress: Diffusion of TikTok and Threads",
          "authors": "Terri L. Towner, Caroline Muñoz",
          "abstract": "This study investigates which members of the 118th U.S. Congress adopt and use Threads and TikTok, and what political, demographic, and constituency-level characteristics explain this variation. Grounded in diffusion of innovation theory, we ask: What factors predict the adoption and use of these emerging platforms? We compiled original data on all members of Congress ( N = 535) by collecting social media account information from official congressional websites and manually verifying platform presence. Adoption was measured as a binary variable, and usage as the number of posts made through November 2023. Using probit and OLS regression models, we tested predictors including party affiliation, age, race, leadership status, and prior digital engagement. The empirical analyses reveal that Democrats and younger legislators are more likely to adopt Threads and TikTok. Prior digital engagement consistently predicts usage on both platforms. Notably, racial identity plays a critical role: non-white members are more likely to adopt and use TikTok, while white members are more likely to use Threads. This study offers the first empirical analysis of congressional adoption and usage of Threads and TikTok. Our findings demonstrate that platform choice is shaped by identity, institutional context, and political strategy. These findings offer new insights into the determinants of early platform adoption among U.S. congress members and the importance of aligning communication choices with constituent behavior and platform culture.",
          "url": "https://doi.org/10.1177/08944393261421118",
          "doi": "https://doi.org/10.1177/08944393261421118",
          "filter": 0,
          "created": "2026-1-30"
        },
        {
          "title": "The Dynamics of Hate Speech: Assessing Anti-Muslim Hate Speech in Norwegian Social Media",
          "authors": "Yuri Kasahara, Daniel Thilo Schroeder, Anis Yazidi, Pedro G. Lind",
          "abstract": "This study investigates the dynamics of anti-Muslim hate speech within Norwegian social media during the period between 2010 and 2021. Using a dataset of more than one million comments from Twitter and Facebook, we developed a custom hate speech classifier trained on an annotated corpus of 3,277 comments in Norwegian language. We identify that despite representing a small share of the total comments, hate speech content has increased over time. In an effort to understand the social network characteristics of hate speech content, we delve deeper into Twitter conversations as we can more easily identify how this content is spread. We develop network metrics to assess the prevalence, distribution, and diffusion of hateful content. The findings reveal that regardless of the number of users or tweets in a conversation, the volume of hateful content tends to remain constant. Furthermore, a small fraction of users contribute disproportionately to the dissemination of hate speech, with most conversations being limited in participant diversity. These results contribute to the growing field of computational social science by offering a novel methodology for studying hate speech in under-resourced languages and suggesting that mitigating hate speech may be possible through targeted network interventions rather than content removal alone.",
          "url": "https://doi.org/10.1177/08944393261417730",
          "doi": "https://doi.org/10.1177/08944393261417730",
          "filter": 0,
          "created": "2026-1-31"
        }
      ],
      "articles_hidden": []
    }
  ]
}
