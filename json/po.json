{
  "update": "2024-11-08",
  "content": [
    {
      "journal_full": "Journal of Elections, Public Opinion and Parties",
      "journal_short": "JEPOP",
      "articles": [
        {
          "title": "Inconsistency of Americans’ opinions on free speech: evidence from three survey experiments",
          "authors": "Kirill Zhirkov, Rachel Smilan-Goldstein",
          "url": "http://dx.doi.org/10.1080/17457289.2024.2421572",
          "doi": "10.1080/17457289.2024.2421572",
          "filter": 0
        },
        {
          "title": "Candidate shortages and the electoral consequences for radical right-wing parties: insights from Sweden",
          "authors": "Zeth Isaksson",
          "url": "http://dx.doi.org/10.1080/17457289.2024.2421566",
          "doi": "10.1080/17457289.2024.2421566",
          "filter": 0
        }
      ],
      "articles_hidden": []
    },
    {
      "journal_full": "Journal of Survey Statistics and Methodology",
      "journal_short": "JSSAM",
      "articles": [
        {
          "title": "Improving the Measurement of Gender in Surveys: Effects of Categorical Versus Open-Ended Response Formats on Measurement and Data Quality Among College Students",
          "authors": "Dana Garbarski, Jennifer Dykema, James A Yonker, Rosie Eungyuhl Bae, Rachel A Rosenfeld",
          "abstract": "While researchers have some recommendations for measuring gender identity in surveys based on research and other sources that are summarized in a series of working group and panel reports, we continue to refine our understanding and practices. Gender identity is usually measured in surveys using a categorical selection response format with a small number of response options (e.g., “female,” “male,” “nonbinary”) and an open text response field to capture additional responses (“not listed, please tell us”). There is limited research guiding researchers on the use of other response formats. This study reports results from a between-subjects experiment embedded in a campus climate survey about university students’ attitudes about their campus and their behaviors and experiences related to inclusion and belonging at a large Midwestern university in 2021. Over 13,000 students were asked “What is your gender?” and subsequently randomly assigned to respond using either a categorical selection response format or an open response format (i.e., a place to specify their gender with no response options listed). We examine the distribution of responses, item nonresponse, response times, and concurrent validity (in terms of the association between gender and relevant survey outcomes) across the two response formats. Findings indicate the categorical selection response format is preferred for this population. While results show similar distributions in the categorization of responses across the formats and similar relationships with other survey outcomes, the selection format is associated with less item nonresponse and shorter response times.",
          "url": "http://dx.doi.org/10.1093/jssam/smae043",
          "doi": "10.1093/jssam/smae043",
          "filter": 0
        }
      ],
      "articles_hidden": []
    },
    {
      "journal_full": "Public Opinion Quarterly",
      "journal_short": "POQ",
      "articles": [
        {
          "title": "Ordering Effects versus Cognitive Burden: How Should We Structure Attributes in Conjoint Experiments?",
          "authors": "Lukas Rudolph, Markus Freitag, Paul W Thurner",
          "abstract": "Conjoint experiments offer a flexible way to elicit population preferences on complex decision tasks. We investigate whether we can improve respondents’ survey experience and, ultimately, choice quality by departing from the current recommendation of completely randomized conjoint attribute ordering. Such random ordering guarantees that potential bias from attribute order cancels out on average. However, in situations with many attributes, this may unnecessarily increase cognitive burden, as attributes belonging together conceptually are presented scattered across the choice table. Hence, we study experimentally whether purposeful ordering (“theoretically important” attributes first) or block randomized ordering (attributes belonging to the same theoretical concept displayed in randomized bundles) affects survey experience, response time, and choice itself, as compared to completely randomized ordering. Drawing on a complex preregistered choice design with nine attributes (N = 6,617), we find that ordering type affects neither self-reported survey experience, choice task timing, nor attribute weighting. Potentially, block randomization reduces cognitive burden for some subgroups. To our knowledge, we thereby provide the first systematic empirical evidence that ordering effects are likely of low relevance in conjoint choice experiments and that the trade-off between cognitive burden and ordering effects is minimal from the perspective of respondents, at least for our substantive application.",
          "url": "http://dx.doi.org/10.1093/poq/nfae038",
          "doi": "10.1093/poq/nfae038",
          "filter": 0
        }
      ],
      "articles_hidden": []
    }
  ]
}
